\section{Existing Work}

\subsection{Eye Tracking and its Applications}
Eye tracking gives information on where do people look and what they ignore. The concept of eye tracking is very basic but the process and implementation can be very diverse and complex (Punde et. al., 2017). Vision is the most important sense (Punde et. al., 2017). In this study, detecting the behavior of drivers when driving especially at night is vital. By using eye tracking technology, the driver’s focus, attention, and presence can be analyzed and studied. The eye trackers are important part of this study because it measure visual attention and has become an important research tool (Jadhav, 2017). The different eye tracker metrics are vital in studying the behavior of the drivers. These metrics include the gaze point, fixation, smooth pursuit and saccades. The gaze point is the basic unit of measuring eye movement. The fixation is a cluster of gaze points. The smooth pursuit is a slow eye movement that allows the eye to follow an object moving slowly. On the other hand, the saccade is a rapid eye movement between fixations. By knowing these metrics, the driver’s eye movements can be studied more. The slow and rapid eye movements of the drivers’ eyes can lead to various inferences and conclusions.
\newline 

Eye tracking is important in many aspects (Krafka et. al., 2016). The number of these aspects grow as technology advances. Even though there are so many aspects wherein eye tracking is applicable, it has yet to become an ubiquitous technology (Krafka et. al., 2016). Meaning, eye tracking technology is not yet widespread or being used globally unlike the technology of voice assistants and speech recognition. The application of eye tracking may range from human-computer interaction, medical diagnoses, psychological studies, and computer vision (Krafka et. al., 2016). Khosla (2016) strongly stated that the human gaze is the externally-observable indicator of human attention. The use of eye tracking requires a device where it will be interfaced for it to reach its full capabilities and functionality. This technology does not require additional devices from users because it can be interfaced on devices that they already have such as mobile phones and computers (Krafka et. al., 2016). A convolutional neural network was interfaced to a mobile device. The results with and without calibration were compared. The results show that a prediction error of 1.34cm to 2.12cm can be obtained with calibration. Without calibration, the error goes up to 1.71cm and 2.53cm.
\newline 

Non-intrusive ways of integrating eye tracking with other devices are better than intrusive ways (Nguyen, 2009). It is because non-intrusive ways do not require devices that cause distraction and annoyance to the user. Intrusive ways include using wearables such as headgears with cameras. On the other hand, non-intrusive ways of eye tracking include having a camera that is from a specific distance away from the user. There were three methods used in predicting eye gaze. These methods include detection using Haar-based features, Lucas Kanade method for tracking eye, and Gaussian process. The Haar-based method used thousands of samples of views and trains classifiers to detect object rapidly (Nguyen, 2009). On the other hand, the Lucas Kanade method uses set of pyramid representations of the original image and use the conventional tracking algorithm for each image pyramid (Nguyen, 2009). Lastly, the Gaussian process was used in eye tracking in such a way that given the gaze data of the user, the mean and covariance of these functions will be used to make predictions. (Nguyen, 2009).

\subsection{Eye Tracking using Different Cameras (RGB-D and Webcam)}
Eye gaze tracking systems are usually based on the use of infrared lights. However, such systems may not work outdoor or may have a very limited head box for them to work (Xiong et, al., 2014). This research paper proposes a non-infrared based approach in order to track the eyes with the use of an RGBD camera using Kinect. Their method uses a personalized 3D face model. The system tracks the center of the iris and a set of 2D facial landmarks whose 3D locations are provided by the camera. (Xiong et, al., 2014). They used a non-IR based method because there are less-strict requirements for hardware and it is easier to integrate with laptops and tablets (Xiong et, al., 2014). Without the use of IR lighting, the visibility of the pupils were decreased, so they replace the task of pupil detection with iris detection. They compared the performance of using RGB from using RGBD and found out that there is more accurate tracking in RGBD. However, the results of their non-IR based method are not as accurate as the ones from the IR based approach. 
\newline

The only RGB-D device used in this project is a Kinect sensor for gaze estimation for humans. The Kinect is set with a head coordinate system where the pupil center is detected from the image. The next step then involves calibration of the eyeball center where the system requests the human to gaze at a position creating a 3D eye model. The direction of the eye gaze is computed with the obtained calibrated eyeball center and pupil center. In their proposed method, it involves three main steps namely the face pose estimation, pupil center estimation, eyeball center calibration, and gaze estimation. The face pose estimation method uses the algorithm provided by Microsoft Kinect where additional devices are not needed to be able to estimate the head pose correctly. In pupil center estimation, an algorithm done by Febian Timm and Erhardt Barth is used that locates the pupil center by assuming that the pupil center P and the eyeball center C is a constant K. The eyeball center calibration is done by having two vectors: the target and the pupil center where the angle is obtained and these parameters are inputted to the given equation to calculate the eyeball center calibration. Lastly, the gaze estimation is obtained by the given parameters from the first three methods mentioned. Their project worked well with minimal error on the accuracy of the algorithm on detecting pupil center. (Jianfeng, L and Shigang, L., 2014).
\newline

In this research, they implemented an eye ball tracking system for patients that cannot perform any voluntary tasks related to daily life. A real time data stream of the eye movement is captured using a webcam that transfers data serially to MATLAB. (Mazhar et, al., 2015). After that, a sequential image processing scheme segments the iris of the eye and calculates the centroid. Their idea was to create a general eye tracking tool that can be interfaced with wheelchairs, moveable patient beds, alerting mechanisms for disabled ones etc. (Mazhar et, al., 2015). The hardware that they used was a webcam by Logitech which can be interfaced serially using a USB port of a computer. This method involves an intrusive system because the camera is mounted to a headband. Their eye detection algorithm involved image acquisition through the use of the webcam, then plane separation which is the RGB plane, then thresholding (segmentation), then open binary image, then structuring, then dilation, and finally centroid calculation.

\subsection{Eye Tracking Applied  on Driver’s Awareness and Eye Behavior}
Road accidents have been prevalent in India and has shown consistency in increasing each year. The highest percentage as to why road accidents occur are due to the drivers themselves. This project aims to determine driver drowsiness by using a webcam in determining the eye blinking action of the driver using an algorithm that is implemented using cascade object identifier from vision toolbox of MATLAB. Their system will take the recorded video intro frames where these frames are then processed one by one, which results to a more accurate eye detection as oppose to eye tracking. To calculate the eye blinking, they have used an equation to calculate average drowsiness, number of closed eye found over number of frames; A closed eye would equate to zero eye blinking and one if the eyes are partially or fully open. If the percentage of drowsiness is set to be more than the threshold value, an alert signal will be generated to the driver. Two systems were used to test the research: System I: Inspirion 15 (DELL), core i3 processor (64bit) with 4GB of RAM, VGA inbuilt web cam, MATLAB 13; and System II: Inspiron 15 (DELL), core 13 processor (64bit) with 8GB of RAM, 3 MP USB web cam, MATLAB 13. About 1000 frames are processed online and 1500 frames are processed offline; while both systems show efficient results in overall drowsiness detection, System II has showed a more promising result as compared to System I. (Ahman, R. and Borole, J., 2015).
\newline

The objective of the project is to be able to simultaneously observe closed eyes and open mouth, when positive for yawning, the driver is alerted with a buzzer. A real-time video is captured by placing a camera in front of the driver using OpenCv, Viola Jones Algorithm, and Contour finding Algorithm are used in face detection. There are multiple detections considered in their system but the four main steps are as follows: face and eye detection, eye blinking detection, mouth detection, and warning system design. To detect face and eyes, they have used Viola Jones and Region of Interest (ROI) to locate the eyes. In eye blinking detection, the researchers only considered detecting the right eye in assumption that both eyes blink at the same time. The method used to get the eye blinking rate uses a ring buffer that calculates eye blinking rate every after first 100 first frames, writing 1 to the buffer when eye blinking is detected and 0 if otherwise. When the mouth is detected, decision of yawning is done by obtaining the difference between largest and smallest Y-coordinate values from the mouth; If the height surpasses the given threshold, the person is yawning. Their experiment lasted for 20 days that is divided into 6 sections: morning, afternoon, a given critical time, evening, night, and another given critical time. They also tested this on different test subjects based on sexuality, a person having or not having a mustaches and/or glasses, and age, giving a general best time result to use the system for each test subject.

\subsection{Pupil Detection Algorithm}
Pupil detection algorithm is an important algorithm in eye tracking. This algorithm has to offer robust, fast, and adaptive process in order to be used efficiently in assistive applications (Pasarica et. al., 2015). These applications include neuromotor patients’ telemonitoring and bidirectional patient-health professional communication. There are various pupil detection algorithms examples of which are based on circular Hough transform (CHT) or Starbust. There are three stages of pupil detection. These include acquisition of the input image, image filtering, and  eye pupil center detection algorithm (Pasarica et. al., 2015). The CHT is considered a two dimensional type of algorithm when the circle radius is already known. The principle of this algorithm relies on the property of edge points of the circle. On the other hand, the Starbust algorithm is a combined model and feature based approach algorithm. It deals with the optical balance between running time and accuracy of detection. For Starbust algorithm to be functional, there are seven stages. These stages are image acquisition, corneal reflection detection, corneal reflection removal, feature points analysis, RANSAC for feature points, contour fit establishment, and model based shape optimization (Pasarica et. al., 2015). The most common technique used by modern eye trackers is Pupil Center Corneal Reflection (PCCR) (Punde et. al., 2017). It uses near-infrared camera or other optical sensor for gaze tracking. Starburst algorithm offers better accuracy in cursor movement with higher interferences (Pasarica et. al., 2015).
\newline

This project uses a non-intrusive eye tracking system which means that it does not cause distraction to the user. There are a lot of road accidents happening everyday and preventing the drowsiness of the driver is a way of reducing road accidents. Their system can detect symptoms of driver fatigue early enough to warn the driver. It will give a warning output in form of a sound and a seatbelt vibration with a frequency of 100 to 300 Hz. (Singh et, al., 2011). They also stated that a good indicator of a fatigue state is in the form of microsleeps which lasts from 2 to 3 seconds. Basically, their method involves having the driver’s image, then face detection, then eye detection, then the calculation of criteria for judging drowsiness. Besides eye tracking, they also have sensors on car steering and the speedometer of the car. These can help in detecting driver drowsiness further than just having eye tracking. However, there is a disadvantage in using their eye tracking system. They are using a computer to process the image of eye movement which is impractical when driving because the user must bring a computer inside the car. Their image processing methods involved the use of MATLAB and it can detect eyelid movement, eye gaze, yarn and head nodding. (Singh et, al., 2011). They based their measure of driver alertness in PERCLOS (Percent Eyelid Closure) as it is the most reliable and valid in a study by the US Federal Highway Administration.

\subsection{Eye Tracking Implementation using Microcontrollers}
The proposed system for eye tracking is implemented in real time using an arduino uno microcontroller and a zigbee wireless device. (Venugopal et, al., 2016). Two types of infrared eye following procedures are used which are enhanced pupil and dim pupil. Their system consists of three general modules. The first module consists of eye tracking and detection in real time using Viola-jones algorithm. The second module involves the detection of the pupils using Hough transformation. The third module  comprises the detection and tracking of the pupils and setting a direction. The obtained direction is sent to the Arduino with a wireless zigbee device for transmission of data. (Venugopal et, al., 2016). Basically, the three modules consists of image pre-processing, pupil region detection and pupil detection with movement detection. 
\newline

The main goal of developing the driver monitoring system using eye gaze tracking is to reduce road accidents and improve road safety. The algorithm detects the drowsiness of the driver and alerts the user using sound and vibration from steering. (Mavely et, al., 2016). The control of the whole framework operation is based on the Raspberry Pi 2. The project uses a Logitech c270 HD webcam and an LED IR illuminator for capturing an image. The camera is connected to the Pi using a USB port and it is placed above the steering wheel. An IR illumination is used to capture the user’s eyes during the night. For eye gaze tracking, CAMSHIFT (Continuously Adaptive Mean Shift) algorithm was used to extract the ROI (Region of Interest). The subject’s drowsiness was detected if the eye gaze is not detected more than 2 secs. Their proposed method was compared with other types of approaches and found out that their method was very cost effective with a high rate of speed and accuracy (Mavely et, al., 2016). They compared it with the bioelectric signal and driver face monitoring methods.


\section{Lacking in the Approaches}
\subsection{Eye Tracking and its Applications}
  For the implementation done by Krafka et. al., eye tracking technology were only integrated to mobile devices. This means that the implementation used a small form factor camera that can only be integrated to small devices like mobile phones. Using GazeCapture as its database, the data is from crowdsourcing (Krafka et. al., 2016). Crowdsourcing was used in order to deviate from prior works which made use of original data from cameras included in their eye trackers. The advantage of having a large database is that models can be tested repetitively for robustness (Krafka et. al., 2016). The eye tracker used  showed results before and after its calibration. The algorithm used in this specific eye tracker can only be used if the data is from a given database. However, Krafka (2016) strongly stated that this eye tracker outperformed state-of-the-art eye trackers by a large margin.
\newline

An introduction made by Punde et. al. (2017) shows that there are two types of eye trackers. These types include remote or screen based eye trackers and head-mounted or mobile eye tracker. The remote eye tracker requires the user to sit in front of the screen to interact with screen based content. On the other hand, head-mounted eye trackers does not distract users and lets them move freely. From their study, there are no direct applications per type of eye trackers and generalizations about both types of eye trackers were presented. This study was included in our research because of applications in automotives industry. Based on the eye trackers’ data, industries may improve their current products and devices. In addition, the visual data may be used for the industries’ analysis of eye movements and behaviors of users such as those of the psychological and medical applications (Punde et. al., 2017). This study is a helpful overview of eye tracking and its applications to different fields and practices. Even though there are no specific target field, the overall use, functionality, advantages, and disadvantages of eye trackers were presented clearly.
\newline

\subsection{Pupil Detection Algorithm}
For the pupil detection algorithm, both the CHT and Starbust algorithms were used using infrared cameras. In addition, no matter how precise these algorithms are, there are other factors that affect the results such as the computing performance of the devices and the visual data from the cameras themselves (Pasarica et. al., 2015). Both algorithms were used comparatively and analyzed based on different conditions that were present. The integration of both algorithms were not presented even though these algorithms are not of the same basis. CHT algorithm is based on parameters while Starbust algorithm is feature-based (Pasarica et. al., 2015). The pupil detection algorithms used were applied to neuromotor disabled patients. The Starbust algorithm was designed in such a way that it can perform better even if there are higher noise levels (Pasarica et. al., 2015). These changes were not applied to CHT algorithm because it uses Gaussian process. 

\section{Summary}

The proponents’ review of related literature were all about eye-tracking technology that comprised of five categories. These are applications of eye tracking, usage of RGB-D camera/Webcam in eye tracking, utilization of driver’s eye behavior in eye tracking , detection of pupil algorithm, and implementation of eye tracking using microcontrollers. 
\newline

In the applications of eye tracking, it was discussed that eye tracking can be used to help persons with disabilities, to study the psychology of people, and to interact with computers easily. In the usage of RGB-D camera/webcam, their differences were seen by their quality of RGBD detection. In the utilization of driver’s eye behavior in eye tracking, it was researched that there were studies to measure driver alertness and it can be based to make an effective eye tracking device for drivers. For the pupil detection algorithm, it was explored that there were a lot of algorithms that can be used for eye tracking. All of those algorithms perform the same task of detecting the pupils but there were variations when doing the math and analysis of the eyes. Lastly, the implementation of eye trackers using microcontrollers were dealt with because the proponents want to focus on having a lighter and compact device for eye detection rather than having laptops/computers as the brains of the eye tracking device. The literatures about these involved different types of microcontrollers like arduino/raspberry pi that were used to implement the eye tracking device.



